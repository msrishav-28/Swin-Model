{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/msrishav-28/Swin-Model/blob/main/Deepfake_Detection_Swin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ufpVFp9ohWk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4392e790-1c6b-46f8-bed9-4706e3fe3479"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download script created successfully!\n"
          ]
        }
      ],
      "source": [
        "# Save the download script to a file\n",
        "!mkdir -p /content/dataset_setup\n",
        "with open('/content/dataset_setup/download_faceforensics.py', 'w') as f:\n",
        "    f.write('''#!/usr/bin/env python\n",
        "\"\"\" Downloads FaceForensics++ and Deep Fake Detection public data release\n",
        "Example usage:\n",
        "    see -h or https://github.com/ondyari/FaceForensics\n",
        "\"\"\"\n",
        "# -*- coding: utf-8 -*-\n",
        "import argparse\n",
        "import os\n",
        "import urllib\n",
        "import urllib.request\n",
        "import tempfile\n",
        "import time\n",
        "import sys\n",
        "import json\n",
        "import random\n",
        "from tqdm import tqdm\n",
        "from os.path import join\n",
        "\n",
        "\n",
        "# URLs and filenames\n",
        "FILELIST_URL = 'misc/filelist.json'\n",
        "DEEPFEAKES_DETECTION_URL = 'misc/deepfake_detection_filenames.json'\n",
        "DEEPFAKES_MODEL_NAMES = ['decoder_A.h5', 'decoder_B.h5', 'encoder.h5',]\n",
        "\n",
        "# Parameters\n",
        "DATASETS = {\n",
        "    'original_youtube_videos': 'misc/downloaded_youtube_videos.zip',\n",
        "    'original_youtube_videos_info': 'misc/downloaded_youtube_videos_info.zip',\n",
        "    'original': 'original_sequences/youtube',\n",
        "    'DeepFakeDetection_original': 'original_sequences/actors',\n",
        "    'Deepfakes': 'manipulated_sequences/Deepfakes',\n",
        "    'DeepFakeDetection': 'manipulated_sequences/DeepFakeDetection',\n",
        "    'Face2Face': 'manipulated_sequences/Face2Face',\n",
        "    'FaceShifter': 'manipulated_sequences/FaceShifter',\n",
        "    'FaceSwap': 'manipulated_sequences/FaceSwap',\n",
        "    'NeuralTextures': 'manipulated_sequences/NeuralTextures'\n",
        "    }\n",
        "ALL_DATASETS = ['original', 'DeepFakeDetection_original', 'Deepfakes',\n",
        "                'DeepFakeDetection', 'Face2Face', 'FaceShifter', 'FaceSwap',\n",
        "                'NeuralTextures']\n",
        "COMPRESSION = ['raw', 'c23', 'c40']\n",
        "TYPE = ['videos', 'masks', 'models']\n",
        "SERVERS = ['EU', 'EU2', 'CA']\n",
        "\n",
        "\n",
        "def parse_args():\n",
        "    parser = argparse.ArgumentParser(\n",
        "        description='Downloads FaceForensics v2 public data release.',\n",
        "        formatter_class=argparse.ArgumentDefaultsHelpFormatter\n",
        "    )\n",
        "    parser.add_argument('output_path', type=str, help='Output directory.')\n",
        "    parser.add_argument('-d', '--dataset', type=str, default='all',\n",
        "                        help='Which dataset to download, either pristine or '\n",
        "                             'manipulated data or the downloaded youtube '\n",
        "                             'videos.',\n",
        "                        choices=list(DATASETS.keys()) + ['all']\n",
        "                        )\n",
        "    parser.add_argument('-c', '--compression', type=str, default='raw',\n",
        "                        help='Which compression degree. All videos '\n",
        "                             'have been generated with h264 with a varying '\n",
        "                             'codec. Raw (c0) videos are lossless compressed.',\n",
        "                        choices=COMPRESSION\n",
        "                        )\n",
        "    parser.add_argument('-t', '--type', type=str, default='videos',\n",
        "                        help='Which file type, i.e. videos, masks, for our '\n",
        "                             'manipulation methods, models, for Deepfakes.',\n",
        "                        choices=TYPE\n",
        "                        )\n",
        "    parser.add_argument('-n', '--num_videos', type=int, default=None,\n",
        "                        help='Select a number of videos number to '\n",
        "                             \"download if you don't want to download the full\"\n",
        "                             ' dataset.')\n",
        "    parser.add_argument('--server', type=str, default='EU',\n",
        "                        help='Server to download the data from. If you '\n",
        "                             'encounter a slow download speed, consider '\n",
        "                             'changing the server.',\n",
        "                        choices=SERVERS\n",
        "                        )\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # URLs\n",
        "    server = args.server\n",
        "    if server == 'EU':\n",
        "        server_url = 'http://canis.vc.in.tum.de:8100/'\n",
        "    elif server == 'EU2':\n",
        "        server_url = 'http://kaldir.vc.in.tum.de/faceforensics/'\n",
        "    elif server == 'CA':\n",
        "        server_url = 'http://falas.cmpt.sfu.ca:8100/'\n",
        "    else:\n",
        "        raise Exception('Wrong server name. Choices: {}'.format(str(SERVERS)))\n",
        "    args.tos_url = server_url + 'webpage/FaceForensics_TOS.pdf'\n",
        "    args.base_url = server_url + 'v3/'\n",
        "    args.deepfakes_model_url = server_url + 'v3/manipulated_sequences/' + \\\n",
        "                               'Deepfakes/models/'\n",
        "\n",
        "    return args\n",
        "\n",
        "\n",
        "def download_files(filenames, base_url, output_path, report_progress=True):\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    if report_progress:\n",
        "        filenames = tqdm(filenames)\n",
        "    for filename in filenames:\n",
        "        download_file(base_url + filename, join(output_path, filename))\n",
        "\n",
        "\n",
        "def reporthook(count, block_size, total_size):\n",
        "    global start_time\n",
        "    if count == 0:\n",
        "        start_time = time.time()\n",
        "        return\n",
        "    duration = time.time() - start_time\n",
        "    progress_size = int(count * block_size)\n",
        "    speed = int(progress_size / (1024 * duration))\n",
        "    percent = int(count * block_size * 100 / total_size)\n",
        "    sys.stdout.write(\"\\\\rProgress: %d%%, %d MB, %d KB/s, %d seconds passed\" %\n",
        "                     (percent, progress_size / (1024 * 1024), speed, duration))\n",
        "    sys.stdout.flush()\n",
        "\n",
        "\n",
        "def download_file(url, out_file, report_progress=False):\n",
        "    out_dir = os.path.dirname(out_file)\n",
        "    if not os.path.isfile(out_file):\n",
        "        fh, out_file_tmp = tempfile.mkstemp(dir=out_dir)\n",
        "        f = os.fdopen(fh, 'w')\n",
        "        f.close()\n",
        "        if report_progress:\n",
        "            urllib.request.urlretrieve(url, out_file_tmp,\n",
        "                                       reporthook=reporthook)\n",
        "        else:\n",
        "            urllib.request.urlretrieve(url, out_file_tmp)\n",
        "        os.rename(out_file_tmp, out_file)\n",
        "    else:\n",
        "        tqdm.write('WARNING: skipping download of existing file ' + out_file)\n",
        "\n",
        "\n",
        "def main(args):\n",
        "    # TOS\n",
        "    print('By pressing any key to continue you confirm that you have agreed '\\\\\n",
        "          'to the FaceForensics terms of use as described at:')\n",
        "    print(args.tos_url)\n",
        "    print('***')\n",
        "    print('Press any key to continue, or CTRL-C to exit.')\n",
        "    _ = input('')\n",
        "\n",
        "    # Extract arguments\n",
        "    c_datasets = [args.dataset] if args.dataset != 'all' else ALL_DATASETS\n",
        "    c_type = args.type\n",
        "    c_compression = args.compression\n",
        "    num_videos = args.num_videos\n",
        "    output_path = args.output_path\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "\n",
        "    # Check for special dataset cases\n",
        "    for dataset in c_datasets:\n",
        "        dataset_path = DATASETS[dataset]\n",
        "        # Special cases\n",
        "        if 'original_youtube_videos' in dataset:\n",
        "            # Here we download the original youtube videos zip file\n",
        "            print('Downloading original youtube videos.')\n",
        "            if not 'info' in dataset_path:\n",
        "                print('Please be patient, this may take a while (~40gb)')\n",
        "                suffix = ''\n",
        "            else:\n",
        "                suffix = 'info'\n",
        "            download_file(args.base_url + '/' + dataset_path,\n",
        "                          out_file=join(output_path,\n",
        "                                        'downloaded_videos{}.zip'.format(\n",
        "                                            suffix)),\n",
        "                          report_progress=True)\n",
        "            return\n",
        "\n",
        "        # Else: regular datasets\n",
        "        print('Downloading {} of dataset \"{}\"'.format(\n",
        "            c_type, dataset_path\n",
        "        ))\n",
        "\n",
        "        # Get filelists and video lenghts list from server\n",
        "        if 'DeepFakeDetection' in dataset_path or 'actors' in dataset_path:\n",
        "            filepaths = json.loads(urllib.request.urlopen(args.base_url + '/' +\n",
        "                DEEPFEAKES_DETECTION_URL).read().decode(\"utf-8\"))\n",
        "            if 'actors' in dataset_path:\n",
        "                filelist = filepaths['actors']\n",
        "            else:\n",
        "                filelist = filepaths['DeepFakesDetection']\n",
        "        elif 'original' in dataset_path:\n",
        "            # Load filelist from server\n",
        "            file_pairs = json.loads(urllib.request.urlopen(args.base_url + '/' +\n",
        "                FILELIST_URL).read().decode(\"utf-8\"))\n",
        "            filelist = []\n",
        "            for pair in file_pairs:\n",
        "                filelist += pair\n",
        "        else:\n",
        "            # Load filelist from server\n",
        "            file_pairs = json.loads(urllib.request.urlopen(args.base_url + '/' +\n",
        "                FILELIST_URL).read().decode(\"utf-8\"))\n",
        "            # Get filelist\n",
        "            filelist = []\n",
        "            for pair in file_pairs:\n",
        "                filelist.append('_'.join(pair))\n",
        "                if c_type != 'models':\n",
        "                    filelist.append('_'.join(pair[::-1]))\n",
        "        # Maybe limit number of videos for download\n",
        "        if num_videos is not None and num_videos > 0:\n",
        "            print('Downloading the first {} videos'.format(num_videos))\n",
        "            filelist = filelist[:num_videos]\n",
        "\n",
        "        # Server and local paths\n",
        "        dataset_videos_url = args.base_url + '{}/{}/{}/'.format(\n",
        "            dataset_path, c_compression, c_type)\n",
        "        dataset_mask_url = args.base_url + '{}/{}/videos/'.format(\n",
        "            dataset_path, 'masks', c_type)\n",
        "\n",
        "        if c_type == 'videos':\n",
        "            dataset_output_path = join(output_path, dataset_path, c_compression,\n",
        "                                       c_type)\n",
        "            print('Output path: {}'.format(dataset_output_path))\n",
        "            filelist = [filename + '.mp4' for filename in filelist]\n",
        "            download_files(filelist, dataset_videos_url, dataset_output_path)\n",
        "        elif c_type == 'masks':\n",
        "            dataset_output_path = join(output_path, dataset_path, c_type,\n",
        "                                       'videos')\n",
        "            print('Output path: {}'.format(dataset_output_path))\n",
        "            if 'original' in dataset:\n",
        "                if args.dataset != 'all':\n",
        "                    print('Only videos available for original data. Aborting.')\n",
        "                    return\n",
        "                else:\n",
        "                    print('Only videos available for original data. '\n",
        "                          'Skipping original.\\\\n')\n",
        "                    continue\n",
        "            if 'FaceShifter' in dataset:\n",
        "                print('Masks not available for FaceShifter. Aborting.')\n",
        "                return\n",
        "            filelist = [filename + '.mp4' for filename in filelist]\n",
        "            download_files(filelist, dataset_mask_url, dataset_output_path)\n",
        "\n",
        "        # Else: models for deepfakes\n",
        "        else:\n",
        "            if dataset != 'Deepfakes' and c_type == 'models':\n",
        "                print('Models only available for Deepfakes. Aborting')\n",
        "                return\n",
        "            dataset_output_path = join(output_path, dataset_path, c_type)\n",
        "            print('Output path: {}'.format(dataset_output_path))\n",
        "\n",
        "            # Get Deepfakes models\n",
        "            for folder in tqdm(filelist):\n",
        "                folder_filelist = DEEPFAKES_MODEL_NAMES\n",
        "\n",
        "                # Folder paths\n",
        "                folder_base_url = args.deepfakes_model_url + folder + '/'\n",
        "                folder_dataset_output_path = join(dataset_output_path,\n",
        "                                                  folder)\n",
        "                download_files(folder_filelist, folder_base_url,\n",
        "                               folder_dataset_output_path,\n",
        "                               report_progress=False)   # already done\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    args = parse_args()\n",
        "    main(args)''')\n",
        "\n",
        "print(\"Download script created successfully!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install timm albumentations pytorch-lightning face_recognition opencv-python-headless scikit-learn matplotlib seaborn\n",
        "\n",
        "# Check GPU\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "4x70ynrnPKLz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e44a75c-38ce-4df5-e7c7-930cc9207a88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.11/dist-packages (1.0.15)\n",
            "Requirement already satisfied: albumentations in /usr/local/lib/python3.11/dist-packages (2.0.6)\n",
            "Collecting pytorch-lightning\n",
            "  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: face_recognition in /usr/local/lib/python3.11/dist-packages (1.3.0)\n",
            "Requirement already satisfied: opencv-python-headless in /usr/local/lib/python3.11/dist-packages (4.11.0.86)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.10.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.13.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from timm) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from timm) (0.21.0+cu124)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from timm) (6.0.2)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (from timm) (0.30.2)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.11/dist-packages (from timm) (0.5.3)\n",
            "Requirement already satisfied: numpy>=1.24.4 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.10.0 in /usr/local/lib/python3.11/dist-packages (from albumentations) (1.15.2)\n",
            "Requirement already satisfied: pydantic>=2.9.2 in /usr/local/lib/python3.11/dist-packages (from albumentations) (2.11.4)\n",
            "Requirement already satisfied: albucore==0.0.24 in /usr/local/lib/python3.11/dist-packages (from albumentations) (0.0.24)\n",
            "Requirement already satisfied: stringzilla>=3.10.4 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (3.12.5)\n",
            "Requirement already satisfied: simsimd>=5.9.2 in /usr/local/lib/python3.11/dist-packages (from albucore==0.0.24->albumentations) (6.2.1)\n",
            "Requirement already satisfied: tqdm>=4.57.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (2025.3.2)\n",
            "Collecting torchmetrics>=0.7.0 (from pytorch-lightning)\n",
            "  Downloading torchmetrics-1.7.1-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (24.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.11/dist-packages (from pytorch-lightning) (4.13.2)\n",
            "Collecting lightning-utilities>=0.10.0 (from pytorch-lightning)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Requirement already satisfied: face-recognition-models>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from face_recognition) (0.3.0)\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from face_recognition) (8.1.8)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.11/dist-packages (from face_recognition) (19.24.6)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from face_recognition) (11.2.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.11/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2022.5.0->pytorch-lightning) (3.11.15)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities>=0.10.0->pytorch-lightning) (75.2.0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.9.2->albumentations) (0.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->timm)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->timm)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->timm)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->timm)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->timm)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->timm)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->timm)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->timm) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface_hub->timm) (2.32.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch-lightning) (1.20.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->timm) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface_hub->timm) (2025.4.26)\n",
            "Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m50.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m74.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.1-py3-none-any.whl (961 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m961.5/961.5 kB\u001b[0m \u001b[31m73.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torchmetrics, pytorch-lightning\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed lightning-utilities-0.14.3 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 pytorch-lightning-2.5.1.post0 torchmetrics-1.7.1\n",
            "Tue May  6 10:24:48 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   58C    P0             26W /   70W |     148MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download FaceForensics++ Dataset with c40 compression (500 videos per method)\n",
        "# This is optimized for T4 GPU in Colab (smaller file size)\n",
        "!python /content/dataset_setup/download_faceforensics.py /content/datasets/faceforensics -d original -c c40 -t videos -n 500 --server EU2\n",
        "!python /content/dataset_setup/download_faceforensics.py /content/datasets/faceforensics -d Deepfakes -c c40 -t videos -n 500 --server EU2\n",
        "!python /content/dataset_setup/download_faceforensics.py /content/datasets/faceforensics -d NeuralTextures -c c40 -t videos -n 500 --server EU2\n",
        "!python /content/dataset_setup/download_faceforensics.py /content/datasets/faceforensics -d Face2Face -c c40 -t videos -n 500 --server EU2"
      ],
      "metadata": {
        "id": "-kia2VpOPLnq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c9ad804-709e-4fde-eda2-d065143df66d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "python3: can't open file '/content/dataset_setup/download_faceforensics.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '/content/dataset_setup/download_faceforensics.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '/content/dataset_setup/download_faceforensics.py': [Errno 2] No such file or directory\n",
            "python3: can't open file '/content/dataset_setup/download_faceforensics.py': [Errno 2] No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y8WWp_DKkaeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "import concurrent.futures\n",
        "import multiprocessing\n",
        "\n",
        "def extract_frames(video_path, output_dir, sample_rate=30):\n",
        "    \"\"\"\n",
        "    Extract frames from a video at the specified sample rate\n",
        "\n",
        "    Args:\n",
        "        video_path: Path to the video file\n",
        "        output_dir: Directory to save extracted frames\n",
        "        sample_rate: Extract 1 frame every 'sample_rate' frames\n",
        "    \"\"\"\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "    video_name = os.path.basename(video_path).split('.')[0]\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "    frame_count = 0\n",
        "    saved_count = 0\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % sample_rate == 0:\n",
        "            frame_path = os.path.join(output_dir, f\"{video_name}_{saved_count:04d}.jpg\")\n",
        "            cv2.imwrite(frame_path, frame)\n",
        "            saved_count += 1\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    return saved_count\n",
        "\n",
        "def process_videos(video_paths, output_dir, sample_rate=30):\n",
        "    \"\"\"Process multiple videos with multiprocessing\"\"\"\n",
        "\n",
        "    # Define a worker function for each video\n",
        "    def worker(video_path):\n",
        "        video_name = os.path.basename(video_path).split('.')[0]\n",
        "        video_output_dir = os.path.join(output_dir, video_name)\n",
        "        return extract_frames(video_path, video_output_dir, sample_rate)\n",
        "\n",
        "    # Get all video paths\n",
        "    all_videos = []\n",
        "    for video_path in video_paths:\n",
        "        if os.path.isdir(video_path):\n",
        "            for root, dirs, files in os.walk(video_path):\n",
        "                for file in files:\n",
        "                    if file.endswith('.mp4'):\n",
        "                        all_videos.append(os.path.join(root, file))\n",
        "        elif video_path.endswith('.mp4'):\n",
        "            all_videos.append(video_path)\n",
        "\n",
        "    print(f\"Found {len(all_videos)} videos to process\")\n",
        "\n",
        "    # Process videos with parallel workers\n",
        "    num_workers = min(multiprocessing.cpu_count(), 4)  # Limit to 4 workers to avoid memory issues\n",
        "    total_frames = 0\n",
        "\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=num_workers) as executor:\n",
        "        # Submit all jobs\n",
        "        future_to_video = {executor.submit(worker, video): video for video in all_videos}\n",
        "\n",
        "        # Process as they complete\n",
        "        for future in tqdm(concurrent.futures.as_completed(future_to_video), total=len(all_videos)):\n",
        "            video = future_to_video[future]\n",
        "            try:\n",
        "                total_frames += future.result()\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {video}: {e}\")\n",
        "\n",
        "    print(f\"Extracted {total_frames} frames in total\")\n",
        "\n",
        "# Define paths for videos and frame extraction\n",
        "ff_original_path = '/content/datasets/faceforensics/original_sequences/youtube/c40/videos'\n",
        "ff_deepfakes_path = '/content/datasets/faceforensics/manipulated_sequences/Deepfakes/c40/videos'\n",
        "ff_neural_path = '/content/datasets/faceforensics/manipulated_sequences/NeuralTextures/c40/videos'\n",
        "ff_face2face_path = '/content/datasets/faceforensics/manipulated_sequences/Face2Face/c40/videos'\n",
        "\n",
        "# Create output directories for extracted frames\n",
        "ff_original_frames = '/content/datasets/frames/faceforensics/original'\n",
        "ff_deepfakes_frames = '/content/datasets/frames/faceforensics/deepfakes'\n",
        "ff_neural_frames = '/content/datasets/frames/faceforensics/neuraltextures'\n",
        "ff_face2face_frames = '/content/datasets/frames/faceforensics/face2face'\n",
        "\n",
        "# Set higher sample rate for T4 GPU (extract fewer frames to manage memory)\n",
        "sample_rate = 60  # Extract 1 frame every 60 frames\n",
        "\n",
        "# Process videos if they exist\n",
        "if os.path.exists(ff_original_path):\n",
        "    print(\"Extracting frames from FaceForensics++ original videos...\")\n",
        "    process_videos([ff_original_path], ff_original_frames, sample_rate)\n",
        "\n",
        "if os.path.exists(ff_deepfakes_path):\n",
        "    print(\"Extracting frames from FaceForensics++ deepfakes videos...\")\n",
        "    process_videos([ff_deepfakes_path], ff_deepfakes_frames, sample_rate)\n",
        "\n",
        "if os.path.exists(ff_neural_path):\n",
        "    print(\"Extracting frames from FaceForensics++ neural textures videos...\")\n",
        "    process_videos([ff_neural_path], ff_neural_frames, sample_rate)\n",
        "\n",
        "print(\"Frame extraction complete!\")"
      ],
      "metadata": {
        "id": "x18T4EQ5PjB8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfe93f9b-6132-4307-f599-c40eb610663f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Frame extraction complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create CelebDF directories\n",
        "!mkdir -p /content/datasets/celebdf/Celeb-real\n",
        "!mkdir -p /content/datasets/celebdf/Celeb-synthesis\n",
        "\n",
        "# Create directories for extracted frames\n",
        "!mkdir -p /content/datasets/frames/celebdf/real\n",
        "!mkdir -p /content/datasets/frames/celebdf/fake"
      ],
      "metadata": {
        "id": "cP36XscAQLsX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GHP0V8t5lLad",
        "outputId": "4981b4e0-0f53-47d4-f0c5-f4ef4e2bfba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install face_recognition\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ayYjZJKlHdUo",
        "outputId": "582695ab-5192-4084-9fcf-12fd977151a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting face_recognition\n",
            "  Downloading face_recognition-1.3.0-py2.py3-none-any.whl.metadata (21 kB)\n",
            "Collecting face-recognition-models>=0.3.0 (from face_recognition)\n",
            "  Downloading face_recognition_models-0.3.0.tar.gz (100.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m100.1/100.1 MB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.11/dist-packages (from face_recognition) (8.1.8)\n",
            "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.11/dist-packages (from face_recognition) (19.24.6)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from face_recognition) (2.0.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from face_recognition) (11.2.1)\n",
            "Downloading face_recognition-1.3.0-py2.py3-none-any.whl (15 kB)\n",
            "Building wheels for collected packages: face-recognition-models\n",
            "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566166 sha256=f963212130733eee9097f41c7e715d49967a288e080c08d9d4ac89a3bb03c6f1\n",
            "  Stored in directory: /root/.cache/pip/wheels/04/52/ec/9355da79c29f160b038a20c784db2803c2f9fa2c8a462c176a\n",
            "Successfully built face-recognition-models\n",
            "Installing collected packages: face-recognition-models, face_recognition\n",
            "Successfully installed face-recognition-models-0.3.0 face_recognition-1.3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import timm\n",
        "import face_recognition\n",
        "from PIL import Image\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import EarlyStopping, ModelCheckpoint\n",
        "import gc\n",
        "\n",
        "# Check PyTorch version and GPU\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ],
      "metadata": {
        "id": "A-z__FViQSRl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80f17fa4-a0d9-48be-f9c9-1fd544d180e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PyTorch version: 2.6.0+cu124\n",
            "CUDA available: True\n",
            "GPU: Tesla T4\n",
            "GPU Memory: 15.83 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    np.random.seed(seed)\n",
        "    import random\n",
        "    random.seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ],
      "metadata": {
        "id": "Dt_2uVIbQq44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FaceExtractor:\n",
        "    \"\"\"Extract and align faces from images\"\"\"\n",
        "\n",
        "    def __init__(self, output_size=224):\n",
        "        self.output_size = output_size\n",
        "\n",
        "    def extract_face(self, image_path):\n",
        "        \"\"\"Extract face from image with alignment\"\"\"\n",
        "        try:\n",
        "            # Check if input is a path or an image array\n",
        "            if isinstance(image_path, str):\n",
        "                # Load image\n",
        "                image = face_recognition.load_image_file(image_path)\n",
        "            else:\n",
        "                # Assume it's already a numpy array\n",
        "                image = image_path\n",
        "\n",
        "            # Find face locations\n",
        "            face_locations = face_recognition.face_locations(image)\n",
        "\n",
        "            if len(face_locations) == 0:\n",
        "                # If no face detected, return resized original image\n",
        "                if isinstance(image_path, str):\n",
        "                    image = cv2.imread(image_path)\n",
        "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "                return cv2.resize(image, (self.output_size, self.output_size))\n",
        "\n",
        "            # Get the first face\n",
        "            top, right, bottom, left = face_locations[0]\n",
        "\n",
        "            # Extract face with some margin\n",
        "            margin = int((bottom - top) * 0.2)\n",
        "            top = max(0, top - margin)\n",
        "            left = max(0, left - margin)\n",
        "            bottom = min(image.shape[0], bottom + margin)\n",
        "            right = min(image.shape[1], right + margin)\n",
        "\n",
        "            face_image = image[top:bottom, left:right]\n",
        "\n",
        "            # Resize to output size\n",
        "            face_image = cv2.resize(face_image, (self.output_size, self.output_size))\n",
        "\n",
        "            return face_image\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image: {str(e)}\")\n",
        "            # Return original image if face extraction fails\n",
        "            if isinstance(image_path, str):\n",
        "                image = cv2.imread(image_path)\n",
        "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            return cv2.resize(image, (self.output_size, self.output_size))"
      ],
      "metadata": {
        "id": "XQJuOT2EQtHo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DeepfakeFrameDataset(Dataset):\n",
        "    def __init__(self, frame_dirs, real_label=0, transform=None, max_samples_per_folder=None):\n",
        "        \"\"\"\n",
        "        Dataset for frame-based deepfake detection\n",
        "\n",
        "        Args:\n",
        "            frame_dirs: List of [directory, label] pairs\n",
        "            real_label: Label value for real images (0 or 1)\n",
        "            transform: Albumentations transforms\n",
        "            max_samples_per_folder: Maximum samples to use per folder (for balancing)\n",
        "        \"\"\"\n",
        "        self.transform = transform\n",
        "        self.image_paths = []\n",
        "        self.labels = []\n",
        "        self.dataset_sources = []\n",
        "\n",
        "        # Collect all image paths and labels\n",
        "        for dir_path, label in frame_dirs:\n",
        "            if not os.path.exists(dir_path):\n",
        "                print(f\"Warning: {dir_path} does not exist, skipping.\")\n",
        "                continue\n",
        "\n",
        "            # Get source dataset from path\n",
        "            if 'faceforensics' in dir_path.lower():\n",
        "                source = 'faceforensics'\n",
        "            elif 'celebdf' in dir_path.lower():\n",
        "                source = 'celebdf'\n",
        "            else:\n",
        "                source = 'unknown'\n",
        "\n",
        "            # Walk through subdirectories for frames\n",
        "            frames_in_folder = []\n",
        "            for root, _, files in os.walk(dir_path):\n",
        "                for file in files:\n",
        "                    if file.endswith(('.jpg', '.png')):\n",
        "                        frames_in_folder.append(os.path.join(root, file))\n",
        "\n",
        "            # Sample frames if needed\n",
        "            if max_samples_per_folder and len(frames_in_folder) > max_samples_per_folder:\n",
        "                frames_in_folder = np.random.choice(frames_in_folder, max_samples_per_folder, replace=False).tolist()\n",
        "\n",
        "            # Add to dataset\n",
        "            self.image_paths.extend(frames_in_folder)\n",
        "            self.labels.extend([label] * len(frames_in_folder))\n",
        "            self.dataset_sources.extend([source] * len(frames_in_folder))\n",
        "\n",
        "        print(f\"Loaded {len(self.image_paths)} images total\")\n",
        "        print(f\"Real images: {self.labels.count(real_label)}\")\n",
        "        print(f\"Fake images: {self.labels.count(1 - real_label)}\")\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Load image\n",
        "        image = cv2.imread(image_path)\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Apply transformations\n",
        "        if self.transform:\n",
        "            augmented = self.transform(image=image)\n",
        "            image = augmented['image']\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "egDMG-9XQuXa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_train_transforms(image_size=224):\n",
        "    return A.Compose([\n",
        "        A.RandomResizedCrop(height=image_size, width=image_size, scale=(0.8, 1.0), ratio=(0.9, 1.1)),\n",
        "        A.HorizontalFlip(p=0.5),\n",
        "        A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1, p=0.5),\n",
        "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])\n",
        "\n",
        "def get_val_transforms(image_size=224):\n",
        "    return A.Compose([\n",
        "        A.Resize(height=image_size, width=image_size),\n",
        "        A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "        ToTensorV2()\n",
        "    ])"
      ],
      "metadata": {
        "id": "30qVFDXmQyTL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SwinDeepfakeDetector(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 model_name='swin_base_patch4_window7_224',\n",
        "                 num_classes=1,\n",
        "                 learning_rate=2e-5,  # Lower learning rate for T4\n",
        "                 weight_decay=1e-5,\n",
        "                 max_epochs=30):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Load pretrained Swin Transformer\n",
        "        self.backbone = timm.create_model(model_name, pretrained=True)\n",
        "\n",
        "        # Modify the classifier head for binary classification\n",
        "        num_features = self.backbone.head.in_features\n",
        "        self.backbone.head = nn.Linear(num_features, num_classes)\n",
        "\n",
        "        # For binary classification\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Metrics storage\n",
        "        self.val_outputs = []\n",
        "        self.test_outputs = []\n",
        "\n",
        "        # Mixed precision - important for T4 GPU\n",
        "        self.use_amp = True\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        # Separate parameters for different learning rates\n",
        "        encoder_params = [p for n, p in self.named_parameters() if 'head' not in n]\n",
        "        classifier_params = [p for n, p in self.named_parameters() if 'head' in n]\n",
        "\n",
        "        optimizer = torch.optim.AdamW([\n",
        "            {'params': encoder_params, 'lr': self.hparams.learning_rate / 10},  # Lower LR for pretrained parts\n",
        "            {'params': classifier_params, 'lr': self.hparams.learning_rate}\n",
        "        ], weight_decay=self.hparams.weight_decay)\n",
        "\n",
        "        # Cosine annealing with warm restarts\n",
        "        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            optimizer,\n",
        "            T_0=10,\n",
        "            T_mult=2,\n",
        "            eta_min=1e-6\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'optimizer': optimizer,\n",
        "            'lr_scheduler': {\n",
        "                'scheduler': scheduler,\n",
        "                'interval': 'epoch',\n",
        "                'frequency': 1\n",
        "            }\n",
        "        }\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        outputs = self(images).squeeze()\n",
        "        loss = self.criterion(outputs, labels.float())\n",
        "\n",
        "        # Calculate metrics\n",
        "        preds = torch.sigmoid(outputs) > 0.5\n",
        "        acc = (preds == labels).float().mean()\n",
        "\n",
        "        # Log metrics\n",
        "        self.log('train_loss', loss, prog_bar=True)\n",
        "        self.log('train_acc', acc, prog_bar=True)\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        outputs = self(images).squeeze()\n",
        "        loss = self.criterion(outputs, labels.float())\n",
        "\n",
        "        # Store outputs for epoch-end metrics\n",
        "        self.val_outputs.append({\n",
        "            'loss': loss,\n",
        "            'preds': torch.sigmoid(outputs),\n",
        "            'labels': labels\n",
        "        })\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_validation_epoch_end(self):\n",
        "        # Aggregate validation outputs\n",
        "        avg_loss = torch.stack([x['loss'] for x in self.val_outputs]).mean()\n",
        "        preds = torch.cat([x['preds'] for x in self.val_outputs])\n",
        "        labels = torch.cat([x['labels'] for x in self.val_outputs])\n",
        "\n",
        "        # Calculate metrics\n",
        "        preds_binary = (preds > 0.5).cpu().numpy()\n",
        "        labels_np = labels.cpu().numpy()\n",
        "        preds_np = preds.cpu().numpy()\n",
        "\n",
        "        acc = accuracy_score(labels_np, preds_binary)\n",
        "        precision = precision_score(labels_np, preds_binary, zero_division=0)\n",
        "        recall = recall_score(labels_np, preds_binary, zero_division=0)\n",
        "        f1 = f1_score(labels_np, preds_binary, zero_division=0)\n",
        "\n",
        "        try:\n",
        "            auc = roc_auc_score(labels_np, preds_np)\n",
        "        except:\n",
        "            auc = 0.0\n",
        "\n",
        "        # Log metrics\n",
        "        self.log('val_loss', avg_loss, prog_bar=True)\n",
        "        self.log('val_acc', acc, prog_bar=True)\n",
        "        self.log('val_precision', precision)\n",
        "        self.log('val_recall', recall)\n",
        "        self.log('val_f1', f1)\n",
        "        self.log('val_auc', auc)\n",
        "\n",
        "        # Clear outputs\n",
        "        self.val_outputs.clear()\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        images, labels = batch\n",
        "        outputs = self(images).squeeze()\n",
        "        loss = self.criterion(outputs, labels.float())\n",
        "\n",
        "        # Store outputs for epoch-end metrics\n",
        "        self.test_outputs.append({\n",
        "            'loss': loss,\n",
        "            'preds': torch.sigmoid(outputs),\n",
        "            'labels': labels\n",
        "        })\n",
        "\n",
        "        return loss\n",
        "\n",
        "    def on_test_epoch_end(self):\n",
        "        # Aggregate test outputs\n",
        "        avg_loss = torch.stack([x['loss'] for x in self.test_outputs]).mean()\n",
        "        preds = torch.cat([x['preds'] for x in self.test_outputs])\n",
        "        labels = torch.cat([x['labels'] for x in self.test_outputs])\n",
        "\n",
        "        # Calculate metrics\n",
        "        preds"
      ],
      "metadata": {
        "id": "Lsg4B_pYQ1Qe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_datasets(max_samples_per_folder=2000):\n",
        "    \"\"\"\n",
        "    Prepare datasets for training, validation, and testing\n",
        "    Balanced to work with T4 GPU memory constraints\n",
        "\n",
        "    Args:\n",
        "        max_samples_per_folder: Maximum samples to use per folder\n",
        "    \"\"\"\n",
        "    # Define dataset directories with labels (0=real, 1=fake)\n",
        "    frame_dirs = [\n",
        "        ['/content/datasets/frames/faceforensics/original', 0],  # FaceForensics++ original (real)\n",
        "        ['/content/datasets/frames/faceforensics/deepfakes', 1],  # FaceForensics++ deepfakes (fake)\n",
        "        ['/content/datasets/frames/faceforensics/neuraltextures', 1],  # FaceForensics++ neural (fake)\n",
        "        ['/content/datasets/frames/celebdf/real', 0],  # CelebDF real\n",
        "        ['/content/datasets/frames/celebdf/fake', 1],  # CelebDF fake\n",
        "    ]\n",
        "\n",
        "    # Create combined dataset - already limiting samples per folder\n",
        "    dataset = DeepfakeFrameDataset(\n",
        "        frame_dirs,\n",
        "        real_label=0,\n",
        "        transform=None,  # No transforms here, will apply later\n",
        "        max_samples_per_folder=max_samples_per_folder\n",
        "    )\n",
        "\n",
        "    # Split dataset maintaining class balance\n",
        "    train_indices, temp_indices = train_test_split(\n",
        "        list(range(len(dataset))),\n",
        "        test_size=0.2,\n",
        "        stratify=dataset.labels,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    val_indices, test_indices = train_test_split(\n",
        "        temp_indices,\n",
        "        test_size=0.5,\n",
        "        stratify=[dataset.labels[i] for i in temp_indices],\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "    # Create image path and label lists for each split\n",
        "    train_paths = [dataset.image_paths[i] for i in train_indices]\n",
        "    train_labels = [dataset.labels[i] for i in train_indices]\n",
        "    train_sources = [dataset.dataset_sources[i] for i in train_indices]\n",
        "\n",
        "    val_paths = [dataset.image_paths[i] for i in val_indices]\n",
        "    val_labels = [dataset.labels[i] for i in val_indices]\n",
        "    val_sources = [dataset.dataset_sources[i] for i in val_indices]\n",
        "\n",
        "    test_paths = [dataset.image_paths[i] for i in test_indices]\n",
        "    test_labels = [dataset.labels[i] for i in test_indices]\n",
        "    test_sources = [dataset.dataset_sources[i] for i in test_indices]\n",
        "\n",
        "    print(f\"Train set: {len(train_paths)} images\")\n",
        "    print(f\"Validation set: {len(val_paths)} images\")\n",
        "    print(f\"Test set: {len(test_paths)} images\")\n",
        "\n",
        "    # Create datasets with appropriate transforms\n",
        "    train_dataset = DeepfakeFrameDataset(\n",
        "        [[train_paths[i], train_labels[i]] for i in range(len(train_paths))],\n",
        "        transform=get_train_transforms(224)\n",
        "    )\n",
        "\n",
        "    val_dataset = DeepfakeFrameDataset(\n",
        "        [[val_paths[i], val_labels[i]] for i in range(len(val_paths))],\n",
        "        transform=get_val_transforms(224)\n",
        "    )\n",
        "\n",
        "    test_dataset = DeepfakeFrameDataset(\n",
        "        [[test_paths[i], test_labels[i]] for i in range(len(test_paths))],\n",
        "        transform=get_val_transforms(224)\n",
        "    )\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset, test_sources\n",
        "\n",
        "def train_model(batch_size=16, max_epochs=30, accumulate_grad_batches=2):\n",
        "    \"\"\"\n",
        "    Train the Swin Transformer model\n",
        "\n",
        "    Args:\n",
        "        batch_size: Batch size for training\n",
        "        max_epochs: Maximum number of training epochs\n",
        "        accumulate_grad_batches: Number of batches to accumulate gradients (helps with small batch sizes)\n",
        "    \"\"\"\n",
        "    # Prepare datasets\n",
        "    train_dataset, val_dataset, test_dataset, test_sources = prepare_datasets(max_samples_per_folder=2000)\n",
        "\n",
        "    # Create dataloaders with appropriate batch sizes\n",
        "    # T4 GPU has ~16GB memory, so we need to be careful with batch size\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=2,\n",
        "        pin_memory=True\n",
        "    )\n",
        "\n",
        "    # Initialize model\n",
        "    model = SwinDeepfakeDetector(\n",
        "        model_name='swin_base_patch4_window7_224',\n",
        "        num_classes=1,\n",
        "        learning_rate=2e-5,  # Lower for T4 GPU\n",
        "        weight_decay=1e-5,\n",
        "        max_epochs=max_epochs\n",
        "    )\n",
        "\n",
        "    # Callbacks\n",
        "    checkpoint_callback = ModelCheckpoint(\n",
        "        monitor='val_f1',\n",
        "        dirpath='checkpoints',\n",
        "        filename='deepfake-detector-{epoch:02d}-{val_f1:.2f}',\n",
        "        save_top_k=3,\n",
        "        mode='max'\n",
        "    )\n",
        "\n",
        "    early_stopping = EarlyStopping(\n",
        "        monitor='val_f1',\n",
        "        patience=5,\n",
        "        mode='max'\n",
        "    )\n",
        "\n",
        "    # Initialize trainer with T4 GPU optimizations\n",
        "    trainer = pl.Trainer(\n",
        "        max_epochs=max_epochs,\n",
        "        accelerator='gpu' if torch.cuda.is_available() else 'cpu',\n",
        "        devices=1,\n",
        "        callbacks=[checkpoint_callback, early_stopping],\n",
        "        precision=16,  # Mixed precision for T4 GPU - CRITICAL for memory efficiency\n",
        "        gradient_clip_val=1.0,\n",
        "        accumulate_grad_batches=accumulate_grad_batches,  # Accumulate gradients to simulate larger batch sizes\n",
        "        deterministic=True,\n",
        "        log_every_n_steps=50,  # Reduce logging frequency to save time\n",
        "        enable_progress_bar=True,\n",
        "        enable_model_summary=True,\n",
        "    )\n",
        "\n",
        "    # Train model\n",
        "    print(\"Starting training...\")\n",
        "    trainer.fit(model, train_loader, val_loader)\n",
        "\n",
        "    # Test model\n",
        "    print(\"Evaluating on test set...\")\n",
        "    test_results = trainer.test(model, test_loader)\n",
        "\n",
        "    return model, test_dataset, test_sources"
      ],
      "metadata": {
        "id": "V1WlFJFcQ5GT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model_by_dataset(model, test_dataset, test_sources, batch_size=16):\n",
        "    \"\"\"\n",
        "    Evaluate model performance on each dataset separately\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        test_dataset: Test dataset\n",
        "        test_sources: Source dataset for each test sample\n",
        "        batch_size: Batch size for evaluation\n",
        "    \"\"\"\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Separate indices by dataset source\n",
        "    ff_indices = [i for i, source in enumerate(test_sources) if source == 'faceforensics']\n",
        "    celebdf_indices = [i for i, source in enumerate(test_sources) if source == 'celebdf']\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Evaluate on FaceForensics++ subset\n",
        "    if ff_indices:\n",
        "        ff_subset = torch.utils.data.Subset(test_dataset, ff_indices)\n",
        "        ff_loader = DataLoader(ff_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "        ff_preds = []\n",
        "        ff_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(ff_loader, desc=\"Evaluating FaceForensics++\"):\n",
        "                images = images.to(device)\n",
        "                outputs = model(images).squeeze()\n",
        "                preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "\n",
        "                ff_preds.extend(preds)\n",
        "                ff_labels.extend(labels.numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        ff_preds = np.array(ff_preds)\n",
        "        ff_labels = np.array(ff_labels)\n",
        "        ff_preds_binary = (ff_preds > 0.5).astype(int)\n",
        "\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(ff_labels, ff_preds_binary),\n",
        "            'precision': precision_score(ff_labels, ff_preds_binary, zero_division=0),\n",
        "            'recall': recall_score(ff_labels, ff_preds_binary, zero_division=0),\n",
        "            'f1': f1_score(ff_labels, ff_preds_binary, zero_division=0),\n",
        "            'auc': roc_auc_score(ff_labels, ff_preds) if len(np.unique(ff_labels)) > 1 else 0.0\n",
        "        }\n",
        "\n",
        "        results['faceforensics'] = {\n",
        "            'metrics': metrics,\n",
        "            'preds': ff_preds,\n",
        "            'labels': ff_labels\n",
        "        }\n",
        "\n",
        "    # Evaluate on CelebDF subset\n",
        "    if celebdf_indices:\n",
        "        celebdf_subset = torch.utils.data.Subset(test_dataset, celebdf_indices)\n",
        "        celebdf_loader = DataLoader(celebdf_subset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "        celebdf_preds = []\n",
        "        celebdf_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for images, labels in tqdm(celebdf_loader, desc=\"Evaluating CelebDF\"):\n",
        "                images = images.to(device)\n",
        "                outputs = model(images).squeeze()\n",
        "                preds = torch.sigmoid(outputs).cpu().numpy()\n",
        "\n",
        "                celebdf_preds.extend(preds)\n",
        "                celebdf_labels.extend(labels.numpy())\n",
        "\n",
        "        # Calculate metrics\n",
        "        celebdf_preds = np.array(celebdf_preds)\n",
        "        celebdf_labels = np.array(celebdf_labels)\n",
        "        celebdf_preds_binary = (celebdf_preds > 0.5).astype(int)\n",
        "\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(celebdf_labels, celebdf_preds_binary),\n",
        "            'precision': precision_score(celebdf_labels, celebdf_preds_binary, zero_division=0),\n",
        "            'recall': recall_score(celebdf_labels, celebdf_preds_binary, zero_division=0),\n",
        "            'f1': f1_score(celebdf_labels, celebdf_preds_binary, zero_division=0),\n",
        "            'auc': roc_auc_score(celebdf_labels, celebdf_preds) if len(np.unique(celebdf_labels)) > 1 else 0.0\n",
        "        }\n",
        "\n",
        "        results['celebdf'] = {\n",
        "            'metrics': metrics,\n",
        "            'preds': celebdf_preds,\n",
        "            'labels': celebdf_labels\n",
        "        }\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\nCross-Dataset Evaluation Results:\")\n",
        "    for dataset, result in results.items():\n",
        "        print(f\"\\n{dataset.upper()}:\")\n",
        "        for metric, value in result['metrics'].items():\n",
        "            print(f\"  {metric}: {value:.4f}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "def plot_confusion_matrices(results):\n",
        "    \"\"\"Plot confusion matrices for each dataset\"\"\"\n",
        "    num_datasets = len(results)\n",
        "    fig, axes = plt.subplots(1, num_datasets, figsize=(6*num_datasets, 5))\n",
        "\n",
        "    if num_datasets == 1:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, (dataset, result) in zip(axes, results.items()):\n",
        "        labels = result['labels']\n",
        "        preds = (result['preds'] > 0.5).astype(int)\n",
        "        cm = confusion_matrix(labels, preds)\n",
        "\n",
        "        sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
        "                    xticklabels=['Real', 'Fake'],\n",
        "                    yticklabels=['Real', 'Fake'], ax=ax)\n",
        "        ax.set_title(f'{dataset.upper()} Confusion Matrix')\n",
        "        ax.set_ylabel('True Label')\n",
        "        ax.set_xlabel('Predicted Label')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('confusion_matrices.png')\n",
        "    plt.show()\n",
        "\n",
        "def plot_roc_curves(results):\n",
        "    \"\"\"Plot ROC curves for each dataset\"\"\"\n",
        "    plt.figure(figsize=(10, 6))\n",
        "\n",
        "    colors = ['darkorange', 'green', 'blue']\n",
        "    for i, (dataset, result) in enumerate(results.items()):\n",
        "        labels = result['labels']\n",
        "        preds = result['preds']\n",
        "\n",
        "        fpr, tpr, _ = roc_curve(labels, preds)\n",
        "        roc_auc = auc(fpr, tpr)\n",
        "\n",
        "        plt.plot(fpr, tpr, color=colors[i], lw=2,\n",
        "                 label=f'{dataset.upper()} (AUC = {roc_auc:.2f})')\n",
        "\n",
        "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
        "    plt.xlim([0.0, 1.0])\n",
        "    plt.ylim([0.0, 1.05])\n",
        "    plt.xlabel('False Positive Rate')\n",
        "    plt.ylabel('True Positive Rate')\n",
        "    plt.title('Receiver Operating Characteristic (ROC) Curves')\n",
        "    plt.legend(loc=\"lower right\")\n",
        "    plt.savefig('roc_curves.png')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "ka5uT8RPRP3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create directories\n",
        "!mkdir -p checkpoints\n",
        "\n",
        "# Clear memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Check available memory on GPU\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB\n",
        "    print(f\"Total GPU memory: {gpu_memory:.2f} GB\")\n",
        "    print(f\"Available GPU memory: {torch.cuda.memory_reserved(0) / 1e9:.2f} GB reserved\")\n",
        "\n",
        "    # Determine batch size based on available memory for T4 GPU\n",
        "    batch_size = 16  # Start with this, will be adjusted if needed\n",
        "    accumulate_grad_batches = 2\n",
        "else:\n",
        "    batch_size = 4\n",
        "    accumulate_grad_batches = 4\n",
        "\n",
        "print(f\"Using batch size: {batch_size} with gradient accumulation: {accumulate_grad_batches}\")\n",
        "\n",
        "# Train model\n",
        "model, test_dataset, test_sources = train_model(\n",
        "    batch_size=batch_size,\n",
        "    max_epochs=30,\n",
        "    accumulate_grad_batches=accumulate_grad_batches\n",
        ")"
      ],
      "metadata": {
        "id": "rwxbgr4kRQ3A",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 512
        },
        "outputId": "97f95236-68ee-4fd5-ef8f-775bcb64c57d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total GPU memory: 15.83 GB\n",
            "Available GPU memory: 0.00 GB reserved\n",
            "Using batch size: 16 with gradient accumulation: 2\n",
            "Warning: /content/datasets/frames/faceforensics/original does not exist, skipping.\n",
            "Warning: /content/datasets/frames/faceforensics/deepfakes does not exist, skipping.\n",
            "Warning: /content/datasets/frames/faceforensics/neuraltextures does not exist, skipping.\n",
            "Loaded 0 images total\n",
            "Real images: 0\n",
            "Fake images: 0\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-6a8ed3bac6dd>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m model, test_dataset, test_sources = train_model(\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mmax_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-d470022e17f0>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(batch_size, max_epochs, accumulate_grad_batches)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \"\"\"\n\u001b[1;32m     85\u001b[0m     \u001b[0;31m# Prepare datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_sources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprepare_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_samples_per_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m     \u001b[0;31m# Create dataloaders with appropriate batch sizes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-d470022e17f0>\u001b[0m in \u001b[0;36mprepare_datasets\u001b[0;34m(max_samples_per_folder)\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;31m# Split dataset maintaining class balance\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     train_indices, temp_indices = train_test_split(\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mtest_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    214\u001b[0m                     )\n\u001b[1;32m    215\u001b[0m                 ):\n\u001b[0;32m--> 216\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInvalidParameterError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m                 \u001b[0;31m# When the function is just a wrapper around an estimator, we allow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36mtrain_test_split\u001b[0;34m(test_size, train_size, random_state, shuffle, stratify, *arrays)\u001b[0m\n\u001b[1;32m   2849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2850\u001b[0m     \u001b[0mn_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marrays\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2851\u001b[0;31m     n_train, n_test = _validate_shuffle_split(\n\u001b[0m\u001b[1;32m   2852\u001b[0m         \u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_test_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2853\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\u001b[0m in \u001b[0;36m_validate_shuffle_split\u001b[0;34m(n_samples, test_size, train_size, default_test_size)\u001b[0m\n\u001b[1;32m   2479\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2480\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_train\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2481\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2482\u001b[0m             \u001b[0;34m\"With n_samples={}, test_size={} and train_size={}, the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2483\u001b[0m             \u001b[0;34m\"resulting train set will be empty. Adjust any of the \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: With n_samples=0, test_size=0.2 and train_size=None, the resulting train set will be empty. Adjust any of the aforementioned parameters."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate model by dataset\n",
        "results = evaluate_model_by_dataset(model, test_dataset, test_sources, batch_size=batch_size)\n",
        "\n",
        "# Visualize results\n",
        "plot_confusion_matrices(results)\n",
        "plot_roc_curves(results)\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), 'deepfake_detector_swin_base.pth')\n",
        "print(\"Model saved successfully!\")\n",
        "\n",
        "# Save to Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "!cp deepfake_detector_swin_base.pth /content/drive/MyDrive/\n",
        "!cp confusion_matrices.png /content/drive/MyDrive/\n",
        "!cp roc_curves.png /content/drive/MyDrive/\n",
        "print(\"Results copied to Google Drive!\")"
      ],
      "metadata": {
        "id": "HlEK5hZTRUBL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "def test_with_random_sample():\n",
        "    \"\"\"Test the model with a random sample from the test dataset\"\"\"\n",
        "    if 'test_dataset' not in globals():\n",
        "        print(\"Test dataset not available. Train the model first.\")\n",
        "        return\n",
        "\n",
        "    # Select a random image from test set\n",
        "    idx = random.randint(0, len(test_dataset)-1)\n",
        "    image, label = test_dataset[idx]\n",
        "\n",
        "    # Convert tensor to numpy for visualization\n",
        "    image_np = image.permute(1, 2, 0).numpy()\n",
        "    image_np = image_np * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "    image_np = np.clip(image_np, 0, 1)\n",
        "\n",
        "    # Make prediction\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        output = model(image.unsqueeze(0).to(device)).squeeze()\n",
        "        prob = torch.sigmoid(output).item()\n",
        "\n",
        "    is_fake = prob > 0.5\n",
        "    confidence = prob if is_fake else 1 - prob\n",
        "    true_label = \"FAKE\" if label == 1 else \"REAL\"\n",
        "    pred_label = \"FAKE\" if is_fake else \"REAL\"\n",
        "\n",
        "    # Display result\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(image_np)\n",
        "    plt.title(f\"True Label: {true_label}\")\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.bar(['Real', 'Fake'], [1-prob, prob], color=['green', 'red'])\n",
        "    plt.ylim(0, 1)\n",
        "    plt.title(f\"Prediction: {pred_label}\\nConfidence: {confidence:.2%}\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    print(f\"True label: {true_label}\")\n",
        "    print(f\"Prediction: {pred_label}\")\n",
        "    print(f\"Confidence: {confidence:.2%}\")\n",
        "    print(f\"Correct prediction: {true_label == pred_label}\")\n",
        "\n",
        "# Run test\n",
        "test_with_random_sample()"
      ],
      "metadata": {
        "id": "Q5YGp5uJRXvR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "def test_with_uploaded_image():\n",
        "    \"\"\"Test the model with an uploaded image\"\"\"\n",
        "    print(\"Please upload an image...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    for filename in uploaded.keys():\n",
        "        # Read image\n",
        "        image = Image.open(io.BytesIO(uploaded[filename]))\n",
        "\n",
        "        # Convert to numpy array\n",
        "        image_np = np.array(image)\n",
        "        if image_np.shape[-1] > 3:  # Handle RGBA\n",
        "            image_np = image_np[:, :, :3]\n",
        "\n",
        "        # Extract face\n",
        "        face_extractor = FaceExtractor(output_size=224)\n",
        "        face = face_extractor.extract_face(image_np)\n",
        "\n",
        "        # Preprocess image\n",
        "        transform = get_val_transforms(224)\n",
        "        image_tensor = transform(image=face)['image'].unsqueeze(0)\n",
        "\n",
        "        # Make prediction\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        model.to(device)\n",
        "        model.eval()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            output = model(image_tensor.to(device)).squeeze()\n",
        "            prob = torch.sigmoid(output).item()\n",
        "\n",
        "        is_fake = prob > 0.5\n",
        "        confidence = prob if is_fake else 1 - prob\n",
        "\n",
        "        # Display result\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.subplot(1, 2, 1)\n",
        "        plt.imshow(face)\n",
        "        plt.title(\"Input Image\")\n",
        "        plt.axis('off')\n",
        "\n",
        "        plt.subplot(1, 2, 2)\n",
        "        plt.bar(['Real', 'Fake'], [1-prob, prob], color=['green', 'red'])\n",
        "        plt.ylim(0, 1)\n",
        "        plt.title(f\"Prediction: {'FAKE' if is_fake else 'REAL'}\\nConfidence: {confidence:.2%}\")\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(f\"Prediction: {'FAKE' if is_fake else 'REAL'}\")\n",
        "        print(f\"Confidence: {confidence:.2%}\")\n",
        "        print(f\"Fake probability: {prob:.2%}\")\n",
        "\n",
        "# Run test with uploaded image\n",
        "# test_with_uploaded_image()"
      ],
      "metadata": {
        "id": "NumK8t7yRaEJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analyze_video(video_path, output_path=None, frame_skip=10):\n",
        "    \"\"\"Analyze a video for deepfake detection\"\"\"\n",
        "\n",
        "    if not os.path.exists(video_path):\n",
        "        print(f\"Video not found at {video_path}\")\n",
        "        return\n",
        "\n",
        "    # Extract face extractor and transforms\n",
        "    face_extractor = FaceExtractor(output_size=224)\n",
        "    transform = get_val_transforms(224)\n",
        "\n",
        "    # Load model\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    # Open video\n",
        "    cap = cv2.VideoCapture(video_path)\n",
        "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "    # Prepare output video if path is provided\n",
        "    if output_path:\n",
        "        fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "        out = cv2.VideoWriter(output_path, fourcc, fps, (width, height))\n",
        "\n",
        "    frame_count = 0\n",
        "    predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        while cap.isOpened():\n",
        "            ret, frame = cap.read()\n",
        "            if not ret:\n",
        "                break\n",
        "\n",
        "            if frame_count % frame_skip == 0:\n",
        "                # Extract face and make prediction\n",
        "                try:\n",
        "                    face = face_extractor.extract_face(frame)\n",
        "                    face_tensor = transform(image=face)['image'].unsqueeze(0).to(device)\n",
        "\n",
        "                    output = model(face_tensor).squeeze()\n",
        "                    prob = torch.sigmoid(output).item()\n",
        "\n",
        "                    is_fake = prob > 0.5\n",
        "                    confidence = prob if is_fake else 1 - prob\n",
        "\n",
        "                    # Add text to frame\n",
        "                    text = f\"{'FAKE' if is_fake else 'REAL'}: {confidence:.2%}\"\n",
        "                    cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                               1, (0, 0, 255) if is_fake else (0, 255, 0), 2)\n",
        "\n",
        "                    predictions.append(prob)\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing frame {frame_count}: {e}\")\n",
        "\n",
        "            if output_path:\n",
        "                out.write(frame)\n",
        "\n",
        "            frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    if output_path:\n",
        "        out.release()\n",
        "\n",
        "    # Calculate overall video prediction\n",
        "    if predictions:\n",
        "        avg_fake_prob = np.mean(predictions)\n",
        "        is_video_fake = avg_fake_prob > 0.5\n",
        "\n",
        "        print(f\"Video Analysis Complete\")\n",
        "        print(f\"Total frames analyzed: {len(predictions)}/{frame_count}\")\n",
        "        print(f\"Average fake probability: {avg_fake_prob:.2%}\")\n",
        "        print(f\"Video verdict: {'FAKE' if is_video_fake else 'REAL'}\")\n",
        "\n",
        "        # Plot prediction histogram\n",
        "        plt.figure(figsize=(10, 4))\n",
        "        plt.hist(predictions, bins=20, alpha=0.7, color='blue')\n",
        "        plt.axvline(x=0.5, color='red', linestyle='--')\n",
        "        plt.axvline(x=avg_fake_prob, color='green', linestyle='-')\n",
        "        plt.title(f\"Frame Predictions Histogram\\nAverage: {avg_fake_prob:.2%}\")\n",
        "        plt.xlabel(\"Fake Probability\")\n",
        "        plt.ylabel(\"Number of Frames\")\n",
        "        plt.savefig('video_analysis.png')\n",
        "        plt.show()\n",
        "\n",
        "        return {\n",
        "            'is_fake': is_video_fake,\n",
        "            'average_probability': avg_fake_prob,\n",
        "            'frame_predictions': predictions\n",
        "        }\n",
        "    else:\n",
        "        print(\"No faces detected in video\")\n",
        "        return None\n",
        "\n",
        "# Example usage - uncomment to run\n",
        "# video_results = analyze_video('/path/to/video.mp4', '/path/to/output.mp4', frame_skip=30)"
      ],
      "metadata": {
        "id": "wjeMtaxURdei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clear memory\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Check memory usage\n",
        "if torch.cuda.is_available():\n",
        "    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9  # GB\n",
        "    reserved = torch.cuda.memory_reserved(0) / 1e9\n",
        "    allocated = torch.cuda.memory_allocated(0) / 1e9\n",
        "    print(f\"Total GPU memory: {gpu_memory:.2f} GB\")\n",
        "    print(f\"Reserved GPU memory: {reserved:.2f} GB\")\n",
        "    print(f\"Allocated GPU memory: {allocated:.2f} GB\")\n",
        "    print(f\"Free GPU memory: {gpu_memory - reserved:.2f} GB\")\n",
        "\n",
        "# Print model size if available\n",
        "if 'model' in globals():\n",
        "    model_size = sum(p.numel() for p in model.parameters()) / 1e6\n",
        "    print(f\"Model size: {model_size:.2f} million parameters\")"
      ],
      "metadata": {
        "id": "pdUUyMqWRhyO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_model_report():\n",
        "    \"\"\"Generate a comprehensive model report\"\"\"\n",
        "\n",
        "    if 'results' not in globals() or 'model' not in globals():\n",
        "        print(\"Results not available. Train the model first.\")\n",
        "        return\n",
        "\n",
        "    # Create report\n",
        "    report = \"\"\"\n",
        "    # Deepfake Detection Model Report\n",
        "\n",
        "    ## Model Architecture\n",
        "    - Backbone: Swin Transformer Base\n",
        "    - Pretrained: ImageNet\n",
        "    - Input Size: 224x224\n",
        "\n",
        "    ## Training Details\n",
        "    - Batch Size: {batch_size}\n",
        "    - Gradient Accumulation: {accumulate_grad_batches}\n",
        "    - Learning Rate: 2e-5 (head), 2e-6 (backbone)\n",
        "    - Mixed Precision: FP16\n",
        "    - Early Stopping: Yes (F1 score)\n",
        "\n",
        "    ## Performance Metrics\n",
        "    \"\"\".format(\n",
        "        batch_size=batch_size,\n",
        "        accumulate_grad_batches=accumulate_grad_batches\n",
        "    )\n",
        "\n",
        "    # Add metrics for each dataset\n",
        "    for dataset, result in results.items():\n",
        "        report += f\"\\n### {dataset.upper()} Dataset\\n\"\n",
        "        for metric, value in result['metrics'].items():\n",
        "            report += f\"- {metric.capitalize()}: {value:.4f}\\n\"\n",
        "\n",
        "    # Save report\n",
        "    with open('model_report.md', 'w') as f:\n",
        "        f.write(report)\n",
        "\n",
        "    # Copy to Drive\n",
        "    !cp model_report.md /content/drive/MyDrive/\n",
        "\n",
        "    print(\"Report generated and saved to Google Drive!\")\n",
        "    return report\n",
        "\n",
        "# Generate report\n",
        "# generate_model_report()"
      ],
      "metadata": {
        "id": "G7cI6l9GRkSZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}